# -*- coding: utf-8 -*-
"""task3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JcduJCwLN3ADF4-rncB8AUlJmN985gd-
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel

class SimpleGPT2(nn.Module):
    def __init__(self, vocab_size, embed_size=768, heads=12, num_layers=12):
        super(SimpleGPT2, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.transformer_blocks = nn.ModuleList([
            nn.TransformerEncoderLayer(embed_size, heads) for _ in range(num_layers)
        ])
        self.transformer = nn.TransformerEncoder(self.transformer_blocks, num_layers=num_layers)
        self.fc_out = nn.Linear(embed_size, vocab_size)

    def forward(self, x, mask):
        x = self.embedding(x)
        x = self.transformer(x)
        x = self.fc_out(x)
        return x

class DummyDataset(torch.utils.data.Dataset):
    def __init__(self, num_samples=1000, seq_length=32):
        self.data = torch.randint(0, 10000, (num_samples, seq_length))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.data[idx]

# Training function for a single GPU
def train_single_gpu(model, train_loader, optimizer, criterion):
    model.train()
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)

        optimizer.zero_grad()
        outputs = model(inputs, mask=None)
        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))
        loss.backward()
        optimizer.step()

# Training function for DDP
def train_ddp(rank, world_size, model, train_loader, optimizer, criterion):
    torch.manual_seed(42)
    device = torch.device(f"cuda:{rank}" if torch.cuda.is_available() else "cpu")
    model = DistributedDataParallel(model.to(device), device_ids=[rank])
    model.train()

    train_loader = torch.utils.data.DataLoader(train_loader.dataset, batch_size=train_loader.batch_size, shuffle=True)

    for epoch in range(epochs):
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)

            optimizer.zero_grad()
            outputs = model(inputs, mask=None)
            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))
            loss.backward()
            optimizer.step()

def train(rank, world_size, model, train_loader, optimizer, criterion):
    if world_size > 1:
        dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)

    if world_size == 1:
        train_function = train_single_gpu
    else:
        train_function = train_ddp if world_size > 1 else train_single_gpu

    train_function(rank, world_size, model, train_loader, optimizer, criterion)

    if world_size > 1:
        dist.destroy_process_group()

if __name__ == '__main__':

    vocab_size = 10000


    model = SimpleGPT2(vocab_size=vocab_size)

    optimizer = optim.AdamW(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    train_dataset = DummyDataset()
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)

    epochs = 5


    world_size = torch.cuda.device_count()

    mp.spawn(train, args=(world_size, model, train_loader, optimizer, criterion), nprocs=world_size, join=True)